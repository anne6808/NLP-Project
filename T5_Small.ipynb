{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anne6808/NLP-Project/blob/main/T5_Small.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before continuing make sure the contents contain the following files found in the drive linked in github: \\\n",
        "modified_squad_data.json \\\n",
        "train-v2.0.json \\\n",
        "\n",
        "These files need to be in the same directory as the sample_data folder and should result in a directory that looks like the following: \\\n",
        "\n",
        "| .. \\\n",
        "| > sample_data \\\n",
        "|  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sample_data_contents \\\n",
        "| train-v2.0 \\\n",
        "| modified_squad_data.json\n",
        "\n"
      ],
      "metadata": {
        "id": "CoaPutjV6z5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cell to install and update all dependencies"
      ],
      "metadata": {
        "id": "Y39FJrrjHv9v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yakPpoP6IaDp"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -U\n",
        "!pip install datasets evaluate\n",
        "!pip install transformers[torch]\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install Cython\n",
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv0HEidlRdBD"
      },
      "source": [
        "Download all required components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcYVTrGhSKgH"
      },
      "source": [
        "Implementation heavily based on the following article: https://medium.com/@ajazturki10/simplifying-language-understanding-a-beginners-guide-to-question-answering-with-t5-and-pytorch-253e0d6aac54"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XO6mpWjK6HRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acbd1b67-f9d4-40a6-cdfc-b428fff88e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import torch\n",
        "import json\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import evaluate  # Bleu\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast, pipeline\n",
        "from nltk.corpus import wordnet as wn\n",
        "from evaluate import load\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Download WordNet if not already downloaded\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  #Here we are defining the tokenizer, model, optimizer, and the Question and Target Length\n",
        "  TOKENIZER = T5TokenizerFast.from_pretrained(\"t5-small\")\n",
        "  MODEL = T5ForConditionalGeneration.from_pretrained(\"t5-small\", return_dict=True)\n",
        "  OPTIMIZER = Adam(MODEL.parameters(), lr=0.00001)\n",
        "  Q_LEN = 256   # Question Length\n",
        "  T_LEN = 32    # Target Length\n",
        "  BATCH_SIZE = 4\n",
        "  DEVICE = \"cpu\""
      ],
      "metadata": {
        "id": "jgNj3Zdb5HTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-yZFL-eDoUo"
      },
      "outputs": [],
      "source": [
        "# Loading the data\n",
        "with open('/content/train-v2.0.json') as f:\n",
        "    data_as_json = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUlzYFXwSuLG"
      },
      "source": [
        "Example of some of the data that is found in the json file. It is not easily accessable right now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pggdepgTGU84"
      },
      "outputs": [],
      "source": [
        "print('context: ' + data_as_json['data'][0]['paragraphs'][0]['context'])\n",
        "print('question: '+ data_as_json['data'][0]['paragraphs'][0]['qas'][0]['question'])\n",
        "print('answer: ' + data_as_json['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnPU8YWgOfT2"
      },
      "outputs": [],
      "source": [
        "# Extracting context, question, and answers from the dataset\n",
        "def prepare_data(data):\n",
        "    articles = []\n",
        "\n",
        "    for article in data[\"data\"]:\n",
        "        for paragraph in article[\"paragraphs\"]:\n",
        "            for qa in paragraph[\"qas\"]:\n",
        "                question = qa[\"question\"]\n",
        "\n",
        "                if not qa[\"is_impossible\"]:\n",
        "                  answer = qa[\"answers\"][0][\"text\"]\n",
        "\n",
        "                inputs = {\"context\": paragraph[\"context\"], \"question\": question, \"answer\": answer}\n",
        "\n",
        "                articles.append(inputs)\n",
        "\n",
        "\n",
        "    return articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0ScTexxOrb5"
      },
      "outputs": [],
      "source": [
        "data = prepare_data(data_as_json)\n",
        "\n",
        "# Create a Dataframe\n",
        "data_as_pd = pd.DataFrame(data)\n",
        "\n",
        "#Limit data to 125. This will create 100 training data and 25 validation data\n",
        "shuffled_data = data_as_pd.sample(frac=1).reset_index(drop=True)[:10]\n",
        "\n",
        "# Print some of the data. Much easier to access and manipulate now\n",
        "shuffled_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd_RpZEAe-2A"
      },
      "source": [
        "Heuristic 1: Changing where the answer is \\\n",
        "In extractive QA models there is the possibility that the model learns that the answer is earlier in the sentence and can skew its effectiveness. It is speculated that generative models should suffer less from this as mentioned in [this](https://arxiv.org/pdf/2004.14602.pdf) article. We can test here by adjusting our contexts. By using only contexts and questions that have the answer in the first half of the sentence we can see if the model is affected by this bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fI8zqgB_iA6F"
      },
      "outputs": [],
      "source": [
        "#This is the function for limiting the data to questions that have the answer in the first half of the context\n",
        "def in_first_half(data):\n",
        "  answer_in_first_half=data[:2000].copy()\n",
        "  indices=[]\n",
        "  for i in data[:2000]['context'].unique():\n",
        "    indices.append(np.where(data_as_pd == i)[0][0])\n",
        "  answer_in_first_half=answer_in_first_half.iloc[indices][:125].reset_index(drop=True)\n",
        "  return answer_in_first_half"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uw2jrbY00Jjb"
      },
      "source": [
        "Heuristic 2: Using synonyms to vary context wording\n",
        "\n",
        "In QA systems, the choice of words within the context can significantly impact the model's performance. By integrating synonyms from lexical resources like WordNet, we aim to diversify the wording of context sentences. This approach enables the model to better understand variations in language and identify relevant information, ultimately improving coverage and robustness. Through experimentation with synonym substitution, we can assess how different word choices influence the model's ability to accurately answer questions, leading to enhanced performance and adaptability across a wide range of queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wbpiKUJDXYr"
      },
      "outputs": [],
      "source": [
        "# Define a function to replace words with synonyms using WordNet\n",
        "def replace_with_synonyms(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    synonyms = []\n",
        "    for token in tokens:\n",
        "        synsets = wn.synsets(token)\n",
        "        if synsets:\n",
        "            synonym = synsets[0].lemmas()[0].name()  # Taking the first synonym from the first synset\n",
        "            synonyms.append(synonym)\n",
        "            # print(f\"Token: {token}, Synonym: {synonym}\")\n",
        "        # else:\n",
        "            # print(f\"No synonym found for token: {token}\")\n",
        "    replaced_text = ' '.join(synonyms)\n",
        "    # print(\"\\nReplaced Text:\", replaced_text)\n",
        "    return replaced_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06hFcrvoS6xb",
        "outputId": "d80c8762-e2e6-434c-c303-ab6c693ea86c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "This is a test sentence with some words.\n",
            "\n",
            "Replaced Text:\n",
            "be angstrom trial sentence some words\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"This is a test sentence with some words.\"\n",
        "\n",
        "# Apply the function to the sample text\n",
        "replaced_text = replace_with_synonyms(sample_text)\n",
        "\n",
        "# Print out the original and replaced text for comparison\n",
        "print(\"Original Text:\")\n",
        "print(sample_text)\n",
        "print(\"\\nReplaced Text:\")\n",
        "print(replaced_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R5nGEeSn9uS7"
      },
      "outputs": [],
      "source": [
        "# This function is used to generate the synonyms dataset\n",
        "def get_synonyms(data):\n",
        "  replaced_with_synonyms=data[:1000].copy()\n",
        "  # Print out the 'context' column of the specified number of rows of the DataFrame\n",
        "  new_context=np.array([replace_with_synonyms(i) for i in data['context'][:1000]])\n",
        "  replaced_with_synonyms['context']=new_context\n",
        "\n",
        "  return replaced_with_synonyms.sample(frac=1).reset_index(drop=True)[:125]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This function adds certain words to the question asked in order to get better results\n",
        "def paraphrase_question(question):\n",
        "    transformations = [\n",
        "        {\n",
        "            'condition': lambda q: q.startswith('What'),\n",
        "            'options': [\n",
        "                lambda q: q.replace('What is', 'What exactly is'),\n",
        "                lambda q: q.replace('What', 'Could you tell me what'),\n",
        "                lambda q: q.replace('What are', 'What exactly are')\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            'condition': lambda q: q.startswith('Where'),\n",
        "            'options': [\n",
        "                lambda q: q.replace('Where', 'Where exactly'),\n",
        "                lambda q: q.replace('Where', 'Could you specify where'),\n",
        "                lambda q: q.replace('Where can', 'Where is it possible to')\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            'condition': lambda q: q.startswith('When'),\n",
        "            'options': [\n",
        "                lambda q: q.replace('When', 'When exactly'),\n",
        "                lambda q: q.replace('When did', 'On what date did'),\n",
        "                lambda q: q.replace('When', 'Can you specify when')\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            'condition': lambda q: q.startswith('Who'),\n",
        "            'options': [\n",
        "                lambda q: q.replace('Who', 'Who exactly'),\n",
        "                lambda q: q.replace('Who was', 'Who was the person who was'),\n",
        "                lambda q: q.replace('Who', 'Can you tell me who')\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            'condition': lambda q: q.startswith('Why'),\n",
        "            'options': [\n",
        "                lambda q: q.replace('Why', 'Can you explain why'),\n",
        "                lambda q: q.replace('Why', 'What are the reasons why'),\n",
        "                lambda q: q.replace('Why did', 'What prompted')\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            'condition': lambda q: q.startswith('How'),\n",
        "            'options': [\n",
        "                lambda q: q.replace('How', 'How exactly'),\n",
        "                lambda q: q.replace('How can', 'In what way can'),\n",
        "                lambda q: q.replace('How do', 'What methods do')\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            'condition': lambda q: 'do you' in q.lower(),\n",
        "            'options': [\n",
        "                lambda q: q.replace('do you', 'does one'),\n",
        "                lambda q: q.replace('Do you', 'Does someone'),\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for rule in transformations:\n",
        "        if rule['condition'](question):\n",
        "            question = random.choice(rule['options'])(question)\n",
        "            break\n",
        "\n",
        "    return question"
      ],
      "metadata": {
        "id": "wKLshWK4gs4a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This takes the json dataset and changes all of the questions\n",
        "def modify_questions(squad_data):\n",
        "    for article in tqdm(squad_data['data']):\n",
        "        for paragraph in article['paragraphs']:\n",
        "            for qa in paragraph['qas']:\n",
        "                original_question = qa['question']\n",
        "                qa['question'] = paraphrase_question(original_question)\n",
        "    return squad_data"
      ],
      "metadata": {
        "id": "Hq-UBgdOgz2d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWaUqo7jKNQy"
      },
      "outputs": [],
      "source": [
        "# This allows us to convert any dataset into a tokenized dataset that can be trained\n",
        "class QA_Dataset(Dataset):\n",
        "    def __init__(self, tokenizer, dataframe, q_len, t_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.q_len = q_len\n",
        "        self.t_len = t_len\n",
        "        self.data = dataframe\n",
        "        self.questions = self.data[\"question\"]\n",
        "        self.context = self.data[\"context\"]\n",
        "        self.answer = self.data['answer']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        context = self.context[idx]\n",
        "        answer = self.answer[idx]\n",
        "\n",
        "        question_tokenized = self.tokenizer(question, context, max_length=self.q_len, padding=\"max_length\",\n",
        "                                                    truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
        "        answer_tokenized = self.tokenizer(answer, max_length=self.t_len, padding=\"max_length\",\n",
        "                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
        "\n",
        "        labels = torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long)\n",
        "        labels[labels == 0] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n",
        "            \"labels\": labels,\n",
        "            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6ss8m-j_KWQ2"
      },
      "outputs": [],
      "source": [
        "# Here we are creating the training and validation splits of the data\n",
        "def create_train(data):\n",
        "  train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "  train_sampler = RandomSampler(train_data.index)\n",
        "  val_sampler = RandomSampler(val_data.index)\n",
        "\n",
        "  qa_dataset = QA_Dataset(TOKENIZER, data, Q_LEN, T_LEN)\n",
        "\n",
        "  train_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "  val_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
        "\n",
        "  return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AXcpqII1L---"
      },
      "outputs": [],
      "source": [
        "# Here we will train the model\n",
        "def train_model(train_loader, val_loader):\n",
        "  # We refresh the tokenizer, model, and optimizer\n",
        "  # This is so we can test new datasets without the previous information interfering and causing a better F1 Score\n",
        "  TOKENIZER = T5TokenizerFast.from_pretrained(\"t5-small\")\n",
        "  MODEL = T5ForConditionalGeneration.from_pretrained(\"t5-small\", return_dict=True)\n",
        "  OPTIMIZER = Adam(MODEL.parameters(), lr=0.00001)\n",
        "\n",
        "  train_loss = 0\n",
        "  val_loss = 0\n",
        "  train_batch_count = 0\n",
        "  val_batch_count = 0\n",
        "  epochs=10\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "  #Training\n",
        "      MODEL.train()\n",
        "      for batch in tqdm(train_loader, desc=\"Training batches\"):\n",
        "          input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "          attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "          labels = batch[\"labels\"].to(DEVICE)\n",
        "          decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
        "\n",
        "          outputs = MODEL(\n",
        "                            input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            labels=labels,\n",
        "                            decoder_attention_mask=decoder_attention_mask\n",
        "                          )\n",
        "\n",
        "          OPTIMIZER.zero_grad()\n",
        "          outputs.loss.backward()\n",
        "          OPTIMIZER.step()\n",
        "          train_loss += outputs.loss.item()\n",
        "          train_batch_count += 1\n",
        "\n",
        "      #Evaluation\n",
        "      MODEL.eval()\n",
        "      for batch in tqdm(val_loader, desc=\"Validation batches\"):\n",
        "          input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "          attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
        "          labels = batch[\"labels\"].to(DEVICE)\n",
        "          decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
        "\n",
        "          outputs = MODEL(\n",
        "                            input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            labels=labels,\n",
        "                            decoder_attention_mask=decoder_attention_mask\n",
        "                          )\n",
        "\n",
        "          OPTIMIZER.zero_grad()\n",
        "          outputs.loss.backward()\n",
        "          OPTIMIZER.step()\n",
        "          val_loss += outputs.loss.item()\n",
        "          val_batch_count += 1\n",
        "\n",
        "          # We also print the loss since it could indicate to us how the model is doing\n",
        "      print(f\"{epoch+1}/{epochs} -> Train loss: {train_loss / train_batch_count}\\tValidation loss: {val_loss/val_batch_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHkZjQuI2Lmg"
      },
      "source": [
        "Here is the evaluation process: \\\n",
        "Use the predict answer in order to generate answers. We do this because the model in its current state does not work well with the processer as shown in homework 3. \\\n",
        "This predict answer function can be used by itself to get a BLEU score as shown in the article that the rest of the model was used from. \\\n",
        "With the predict function we can alter the code given in homework 3 to generate answers with our model and test it against SQuAD validation set. This also prints out the f1 model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5TZspuOeobjU"
      },
      "outputs": [],
      "source": [
        "def predict_answer(context, question, ref_answer=None):\n",
        "    #Tokenizer converts the context and question into tokenized values\n",
        "    inputs = TOKENIZER(question, context, max_length=256, padding=\"max_length\", truncation=True, add_special_tokens=True)\n",
        "\n",
        "    input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to('cpu').unsqueeze(0)\n",
        "    attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to('cpu').unsqueeze(0)\n",
        "\n",
        "\n",
        "    #The model now predicts the value given the context and the Tokenizer decodes it\n",
        "    outputs = MODEL.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    predicted_answer = TOKENIZER.decode(outputs.flatten(), skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    #If a refrence answer is given, we can compare the reference to the predicted. If not, it will return the predicted answer in text\n",
        "    if ref_answer:\n",
        "        # Load the Bleu metric\n",
        "        bleu = evaluate.load(\"google_bleu\")\n",
        "        score = bleu.compute(predictions=[predicted_answer],\n",
        "                            references=[ref_answer])\n",
        "\n",
        "        print(\"Context: \\n\", context)\n",
        "        print(\"\\n\")\n",
        "        print(\"Question: \\n\", question)\n",
        "        return {\n",
        "            \"Reference Answer: \": ref_answer,\n",
        "            \"Predicted Answer: \": predicted_answer,\n",
        "            \"BLEU Score: \": score\n",
        "        }\n",
        "    else:\n",
        "      return predicted_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ6OAq_lrUJv"
      },
      "outputs": [],
      "source": [
        "squad_dataset = load_dataset('squad', split='validation') # Makes the process of loading datasets much easier than before\n",
        "squad_dataset = squad_dataset.select(random.choices([i for i in range(len(squad_dataset))], k=1000))\n",
        "squad_evaluate = load('squad')\n",
        "\n",
        "def evaluate_hf_model(MODEL, model_name):\n",
        "    model = MODEL       # Initialize the model\n",
        "    tokenizer = TOKENIZER                   # Initialize the tokenizer\n",
        "\n",
        "    def dataset_generator(dataset):\n",
        "        for ex in dataset:\n",
        "            yield (ex,\n",
        "                {'question' : ex['question'], 'context': ex['context']})\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    # For prediction text we no longer use the pipeline but instead use our predict function\n",
        "    for ex in tqdm(dataset_generator(squad_dataset), total=len(squad_dataset)):\n",
        "        predictions.append({\n",
        "\n",
        "                'id' : ex[0]['id'],\n",
        "                'prediction_text' : predict_answer(ex[0]['context'], ex[0]['question'])\n",
        "        }\n",
        "        )\n",
        "\n",
        "        # In each example, there are multiple possible answers which we compare to. Here we are converting from them from the datasets format to the one expected by the evaluation metric.\n",
        "        references.append({\n",
        "            'id' : ex[0]['id'],\n",
        "            'answers' : [{'text' : z[0], 'answer_start' : z[1]} for z in zip(ex[0]['answers']['text'], ex[0]['answers']['answer_start'])]\n",
        "        })\n",
        "\n",
        "    # Compute metrics\n",
        "    squad_evaluate.compute(predictions=predictions, references=references)[\"f1\"]\n",
        "    print('Performance of {} : {}'.format(model_name, squad_evaluate.compute(predictions=predictions, references=references)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mvZxhTJu9ec"
      },
      "outputs": [],
      "source": [
        "# This generates a model trained on the baseline data. This will also generate a F1 Score\n",
        "train, val = create_train(shuffled_data)\n",
        "train_model(train, val)\n",
        "evaluate_hf_model(MODEL, 'Baseline')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This generates a model trained oon the synonyms dataset. We also evaluate it with a F1 Score\n",
        "synonyms=get_synonyms(data_as_pd)\n",
        "train, val = create_train(synonyms)\n",
        "train_model(train, val)\n",
        "evaluate_hf_model(MODEL, 'Synonyms')"
      ],
      "metadata": {
        "id": "tv2zAXoOOYoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we are altering the SQuAD dataset with the prompting features\n",
        "question_changed_data=modify_questions(data_as_json)\n",
        "question_changed_pd=prepare_data(question_changed_data)\n",
        "question_changed_pd=pd.DataFrame(question_changed_pd)\n",
        "question_changed_pd = question_changed_pd.sample(frac=1).reset_index(drop=True)[:125]"
      ],
      "metadata": {
        "id": "cJLw_Y_7gQGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We train and evaluate the model on the new prompting dataset\n",
        "train, val = create_train(question_changed_pd)\n",
        "train_model(train, val)\n",
        "evaluate_hf_model(MODEL, 'Prompting')"
      ],
      "metadata": {
        "id": "UycmWV15OY0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We get the dataset with the altered entites dataset and prepare it similarly to the baseline\n",
        "with open('/content/modified_squad_data.json') as f:\n",
        "    altered_entities = json.load(f)"
      ],
      "metadata": {
        "id": "PhDJfNdY3OqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "altered = prepare_data(altered_entities)\n",
        "\n",
        "# Create a Dataframe\n",
        "altered_pd = pd.DataFrame(altered)\n",
        "\n",
        "#Limit data to 125. This will create 100 training data and 25 validation data\n",
        "altered_pd = altered_pd.sample(frac=1).reset_index(drop=True)[:10]\n"
      ],
      "metadata": {
        "id": "oCZo0Iki5nmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate the model on the altered entities dataset\n",
        "train, val = create_train(altered_pd)\n",
        "train_model(train, val)\n",
        "evaluate_hf_model(MODEL, 'Altered Entities')"
      ],
      "metadata": {
        "id": "ngtoQlS5OZM-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}